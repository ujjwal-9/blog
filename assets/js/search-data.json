{
  
    
        "post0": {
            "title": "Introduction to Pydicom",
            "content": "PyDICOM Website . Code can be found @ Github. Checkout dicom-viewer.ipynb . Abstract . Pydicom is a pure Python package for working with DICOM files. It lets you read, modify and write DICOM data in an easy ‚Äúpythonic‚Äù way. . Introduction . Installation . Using pip: . $ pip install pydicom . Using conda: . $ conda install -c conda-forge pydicom . Pydicom comes with its own set of dicom images which can be used to go through examples. . They also give get_dataset.py file to download datasets, which is also included in the github repo. . $ python get_datasets.py --show $ python get_datasets.py --output {path} . Tutorial . def load_scan(path): slices = [pydicom.dcmread(path + &#39;/&#39; + s) for s in os.listdir(path)] slices = [s for s in slices if &#39;SliceLocation&#39; in s] slices.sort(key = lambda x: int(x.InstanceNumber)) try: slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2]) except: slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation) for s in slices: s.SliceThickness = slice_thickness return slices . load_scan loads the dicom files and sorts them according to their Instance Number. It also extracts slice thickness which is a very important parameter in these scans as it is indicative of resolution of the scans. Lower the slice thickness, better the resolution. . Slice Thickness . Slice thickness directly impacts the precision of target localization during treatment. . Slice Increment/Spacing refers to the movement of the table/scanner for scanning the next slice. . If slice thickness is greater than slice increment than there is anatomical information loss. . If there is overlap between 2 adjacent slices that is slice thickness &gt; slice increment than such cases acts as error correction. . HU Scaling . def get_pixels_hu(scans): image = np.stack([s.pixel_array for s in scans]) image = image.astype(np.int16) # Set outside-of-scan pixels to 0 # The intercept is usually -1024, so air is approximately 0 image[image == -2000] = 0 # Convert to Hounsfield units (HU) intercept = scans[0].RescaleIntercept slope = scans[0].RescaleSlope if slope != 1: image = slope * image.astype(np.float64) image = image.astype(np.int16) image += np.int16(intercept) return np.array(image, dtype=np.int16) . HU scaling is explained in my dicom standard blog. . Multiplanar reconstruction . . slices = sorted(patient_dicom, key=lambda s: s.SliceLocation) # pixel aspects, assuming all slices are the same ps = slices[0].PixelSpacing ss = slices[0].SliceThickness ax_aspect = ps[1]/ps[0] sag_aspect = ps[1]/ss cor_aspect = ss/ps[0] # create 3D array img_shape = list(slices[0].pixel_array.shape) img_shape.append(len(slices)) img3d = np.zeros(img_shape) # fill 3D array with the images from the files for i, s in enumerate(slices): img2d = s.pixel_array img3d[:, :, i] = img2d . Multiplanar reformation or reconstruction (MPR) involves the process of converting data from an imaging modality acquired in a certain plane, usually axial, into another plane. It is most commonly performed with thin-slice data from volumetric CT in the axial plane, but it may be accomplished with scanning in any plane and whichever modality capable of cross-sectional imaging, including magnetic resonance imaging (MRI), PET and SPECT. . Source . Windowing . level = dicom_file.WindowCenter width = dicom_file.WindowWidth # ...or set window/level manually to values you want vmin = level - width/2 vmax = level + width/2 plt.imshow(hu_pixels, cmap=&#39;gray&#39;, vmin=vmin, vmax=vmax) plt.show() . . Brain windows are useful for evaluation of brain hemorrhage, fluid-filled structures including blood vessels and ventricles, and air-filled spaces. . Bone windows are useful for evaluation in the setting of trauma. . References . ‚ÄúMartin J. Murphy. The importance of computed tomography slice thickness in radiographic patient positioning for radiosurgery. ‚Äú . ‚ÄúWhat is the difference between slice thickness and slice increment?‚Äù . ‚ÄúHounsfield Scale‚Äù . ‚ÄúUsefulness of hounsfield unit and density in the assessment and treatment of urinary stones‚Äù . ‚ÄúMultiplanar reformation (MPR)‚Äù. Dr Daniel J Bell and Dr Francis Fortin et al. radiopaedia.org . . Follow me on twitter @theujjwal9 .",
            "url": "http://ujjwal9.ml/blog/medicine/library/2020/12/31/pydicom-tutorial.html",
            "relUrl": "/medicine/library/2020/12/31/pydicom-tutorial.html",
            "date": " ‚Ä¢ Dec 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to DICOM Standard",
            "content": "Dicom Website! . DICOM = Digital Imaging and Communications in Medicine . DICOM standard includes a file format definition and a network communications protocol that uses TCP/IP to communicate between systems. . Abstract . Dicom standard was conceptualized and implemented when CT (computed tomography) Scans were developed. It takes special care of not losing the information when translating the medical images and other data to digital format. This is done so as to keep it as close to original as possible. . The main objective of this new standard was to create an open platform for the communication of medical images and related data. . Introduction . The contents of the DICOM standard go far beyond a definition of an exchange format for medical image data. . DICOM defines: . data structures (formats) for medical images and related data, | network oriented services, e. g. image transmission | query of an image archive (PACS) | print (hardcopy) | RIS - PACS - modality integration | . | formats for storage media exchange | requirements for conforming devices and programs | . Image Source . PACS (Picture Archiving and Communication System) provides economical storage of, and convenient access to, images from multiple modalities (source machine types). . RIS (Radiology Information System) is a computerized database used by radiology departments to store, manipulate, and distribute patient radiological data and imagery. . Image Source . PACS-RIS integration improves the flow of images for the radiologist. They communicate using some set of commands (called HL7) concerned with Admission/Discharge/Transfer (ADT) and Order/Entry. . DICOM Data Structures . Dicom consists of list of image attributes which contain vast amount of image and medical information: . patient information (name, sex, identification number) | modality and imaging procedure information (device parameters, calibration, radiation dose, contrast media) | image information (resolution, windowing) | . Dicom goes to the lengths and defines significance of each data element in multitude of cases. It defines if an attribute is required, optional or important for certain cases. But this feature comes at a cost. . Image objects are frequently incomplete : There is inconsistency in filling all the fields with the data. Some fields in image objects are often are left blank and some are filled with incorrect data. | Another problem occurs when displaying an image on a device that is made from different manufacturer, because different imaging equipment use different amplitude ranges and the same number of allocated bits. In that case, images can be displayed as underexposed or overexposed with poor contrast, so those parameters should be adjusted manually. | DICOM Network Services . This service is based over client-sever concept. They (Dicom applications) establish connection to exchange information. In addition to image transmission, there are other features too: . Image Archive Service: search images in a PACS archive by certain criteria (patient, time of creation of the images, modality etc.) and to selectively download images from this archive. | Print Service: gives access to cameras and printers over a network. | Modality Worklist Service: Download updated information regarding patient using above decribed PACS-RIS system. | . Patient Model . Queries to image archives (PACS) are made in 4 level of DICOM hierarchy: . DICOM Data Model Patient has studies, studies have series which are scans and scans may have multiple instances or images (which are slices in CT scans). . Patient Level How many studies are there for this patient | How many Series are there for this patient (in all studies) | How many Instances (images) are there for this patient (in all series) | . | Study Level (Patient and Study roots) How many Series are there in this study | How many Instances (images) are there in this study (in all series) | . | Series Level How many Instances (images) are there in this series | . | Instance Level | Attribute Name Tag Attribute Description . Number of Patient Related Studies | (0020,1200) | The number of studies that match the Patient level Query/Retrieve search criteria | . Number of Patient Related Series | (0020,1202) | The number of series that match the Patient level Query/Retrieve search criteria | . Number of Patient Related Instances | (0020,1204) | The number of composite object instances that match the Patient level Query/Retrieve search criteria | . Number of Study Related Series | (0020,1206) | The number of series that match the Study level Query/Retrieve search criteria | . Number of Study Related Instances | (0020,1208) | The number of composite object instances that match the Study level Query/Retrieve search criteria | . Number of Series Related Instances | (0020,1209) | The number of composite object instances in a Series that match the Series level Query/Retrieve search criteria | . SOP Classes in Study | (0008,0062) | The SOP Classes contained in the Study. | . Above table is sourced from here . Media Exchange . DICOM defines application profiles which defines how media is exchanged: . Encoding formats and compression schemes used (e. g. only uncompressed or loss-less JPEG) | Storage medium used | Images from which modalities may be present on the medium (X-Ray Angiography images, etc) | . DICOM directory: each dicom medium contains this directory which contains information (patient name, modality, unique identifiers etc.) for all images which are captured on the medium. . Device Conformance . Each DICOM supporting device must also specify which DICOM services and options are supported, which extensions and peculiarities have been implemented by the vendor, and how the device communicates with other DICOM systems. . DICOM File Format . The DICOM standard is divided in 2 parts: . A DICOM file consists of a header and image data sets. . Preamble is used to access the images and other data in DICOM file. . Prefix contains the string ‚ÄúDICM‚Äù as uppercase characters. . Data Set is the representation of real world information. . Data Elements. There are 5 types of Data elements: . Type 1 Required Data elements, | Type 1C Conditional Data Elements, | Type 2 Required Data Elements, | Type 2C Conditional Data Elements | Type 3 optional Data Elements. | . Clinical Terms . Some clinical terms to be aware of! . Slice Thickness . Slice thickness refers to the (often axial) resolution of the scan. . Patient Position (0018, 5100) will tell you if the patient was scanned head-first supine, feet-first prone, head-first prone, etc. Instance Number (0020, 0013), also commonly known as slice number, contains no information about spatial location and isn‚Äôt even guaranteed to be unique. Slice Location (0020, 1041) is useful, if it exists, but you can‚Äôt count on it always existing because it‚Äôs a Type 3 (optional) attribute. To have a robust solution, you need to use Image Position Patient (0020, 0032) together with Image Orientation Patient (0020, 0037) and Patient Position (0018, 5100) to properly order the slices in space. Image Position Patient gives you a vector from the origin to the center of the first transmitted pixel of the image. Image Orientation Patient gives you vectors for the orientation of the rows and columns of the image in space. Patient Position tells you how the patient was placed on the table relative to the coordinate system. . Sourced from stackoverflow . Hu Scaling (Hounsfield scale) . HU is a quantitative scale for describing radiodensity. HU‚Äôs is standardized across all CT scans regardless of the scanner detector. . HU=1000‚àóŒº‚àíŒºwaterŒºwater‚àíŒºairHU = 1000 * frac{ mu - mu_{water}}{ mu_{water} - mu_{air}}HU=1000‚àóŒºwater‚Äã‚àíŒºair‚ÄãŒº‚àíŒºwater‚Äã‚Äã . Where $ mu$ is linear attenuation coefficient. . Substance HU . Air | ‚àí1000 | . Fat | ‚àí120 to ‚àí90 | . Soft tissue on contrast CT | +100 to +300 | . Bone | Cancellous | +300 to +400 | . Cortical | +1800 to +1900 | . Here‚Äôs a quick list of a few useful ones, sourced from Wikipedia. . Windowing . Since it is difficult to recognize 4000 shades of gray easily, we use windowing. It limits the number of Hounsfield units that are displayed. . For example, if we want to examine the soft tissue in one CT scan we can use a window level of 40 and a window Width of 80 this will cover 40 units below and above the window level and the tissues with CT numbers outside this range will appear either black or white. A narrow range provides a higher contrast. . The window width is the range of the grayscale that can be displayed. The center of grayscale range is referred to as the window level. . You can go throught this blog to learn more about windowing. . Walkthrough Ohif Viewer . This section contains steps on how to use ohif viewer for viewing dicom file based medical records (studies, series, instances). . Homepage . Here you see the studies you have uploaded. Each study have series and these series have instances / images. . Studies can be uploaded using + button on right. Example is shown below. . Details on Scan View . Calibrating Windowing . Vertical mouse movement changes window level and horizontal movement changes window width. . Visualization of slices . 2D Multiplanar Reconstruction (MPR) . The MPR tool provided within the Viewer can be used to reconstruct images in orthogonal planes (coronal, sagittal, axial or oblique, depending on what the base image plane is). This can help to create a visualization of the anatomy which was not possible using base images alone. . Report view . Report view has report on doc/pdf format displayed in viewer. . References . ‚ÄúSignificance of digital imaging and communication in medicine in digital imaging‚Äù. digitmedicine.com | Dicom Website | Dicom @ OFFIS | ‚ÄúPicture archiving and communication system‚Äù. medico-eng.com | ‚ÄúThe case for RIS/PACS integration‚Äù. PUBMED NCBI | ‚ÄúCounting Studies, Series and Instances‚Äù. medicalconnections.co.uk | . . Follow me on twitter @theujjwal9 .",
            "url": "http://ujjwal9.ml/blog/medicine/2020/12/28/dicom-intro.html",
            "relUrl": "/medicine/2020/12/28/dicom-intro.html",
            "date": " ‚Ä¢ Dec 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Docker",
            "content": "Docker Website! . Checkout my project code @ Github . Abstract . We often try to simplify things but usually end up making it much more difficult. Similar is the case with code. We code, install additional dependencies, and remove redundancies. With this 3 step process, we sometimes end up with very difficult process to explain on how to reproduce the results and rerun the experiments. This blog explains about docker which is a tool designed to make it easier to create, deploy, and run applications by using containers. . Earlier Work . Before docker was introduced, virtualization of resources was used which provided independent virtual machines for clients to work upon. But this came with a price of heavy operating systems which may easily exceed over 1GB despite supporting light applications (like 300MB). So, this drawback led to advent of containers (dockers). . What is docker? . Docker is based on containers which run on shared resources of your PC but in isolation as shown in the following architecture. Container is an efficient mechanism to keep your software components together and maintainable. You can also run multiple containers at the same time to support a serive. Docker also provides with a mechanism to start all the containers concerned with that service with one command using docker compose. We will talk about it later. . Dockerfile . A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. . Working Dockerfile for conda environemnt. . FROM continuumio/miniconda3 WORKDIR /app # Create the environment: COPY environment.yml . RUN conda env create -f environment.yml # Make RUN commands use the new environment: SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;myenv&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;] RUN python -c &quot;import numpy&quot; # The code to run when container is started: COPY run.py . ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;myenv&quot;, &quot;python&quot;, &quot;run.py&quot;] . FROM creates a layer from the continuumio/miniconda3 Docker image. . | COPY adds files from your Docker client‚Äôs current directory. . | RUN builds your application with make. . | CMD specifies what command to run within the container. . | ENTRYPOINT is to set the image‚Äôs main command, allowing that image to be run as though it was that command. . | SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [&quot;/bin/sh&quot;, &quot;-c&quot;], and on Windows is [&quot;cmd&quot;, &quot;/S&quot;, &quot;/C&quot;] . | . Container Image is built using . $ sudo docker build -t &lt;app_name&gt;:&lt;label_name&gt; . . To make sure certain package is installed we can add, . RUN echo &quot;Make sure flask is installed:&quot; RUN python -c &quot;import flask&quot; . The image defined by your Dockerfile generate containers that have ephemeral states. It gets destroyed as soon as process is over. To access files in container we may either bash into container to run the command which generates a new file which we need on our local file system and then use $docker cp to tranfer to local file system. And if we wish to use files from our local system in docker container we may mount those files either using COPY when generating container or we may mount it at runtime using volume. We can also specify volume which can be utilized by both local file system and docker. . $ sudo docker build -t dockerized-run . $ sudo docker run --rm -it -v &lt;PATH-TO_IMAGES&gt;/images:/app/images --entrypoint=/bin/bash dockerized-run (base) root@b74706db6f68:/app# ls environment.yml images (base) root@b74706db6f68:/app# cd images (base) root@b74706db6f68:/app/images# ls out.png test.png . Volume . Mounting volume at runtime. | . $ sudo docker run --rm -it -v &lt;source-path&gt;:&lt;target-path&gt; &lt;docker-container-name&gt; . The above command will run docker with specified volume (-v) mounted in the . Mounting volume at build time using docker compose. | . version: &quot;3.9&quot; services: deeplearning: build: . volumes: - .:/app . Here volume keyword specifies to mount current directory on local file system to /images on container. So the changes made to those files mounted at /images will also be reflected in local file system. . Docker Compose . It is used to start multiple containers as a single service. You may start services like react and flask server together as a service. . version: &quot;3.9&quot; services: web: build: . ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; . Taken from docker compose example at docker compose docs. Here we are starting 2 services web and redis. Web is build using dockerfile as specified by . (dot) pointing towards dockerfile and port binds the container and the host machine to the exposed port, 5000. This can also be done using dockerfile by using EXPOSE. . version is used to specify that we want the details of the version of Docker Compose. . Application . Lets also talk about dockers application in AI research. Now given todays deep learning systems and its other applications, the need for using sameversion of library becomes necessary for inducing reproducibilty in these models. The most common package manager used for python is anaconda. . name: myenv channels: - conda-forge dependencies: - python=3.8 - numpy . We build it with below specified dockerfile. . FROM continuumio/miniconda3 COPY environment.yml . RUN conda env create -f environment.yml ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;example&quot;, &quot;python&quot;, &quot;-c&quot;, &quot;import numpy; print(&#39;success!&#39;)&quot;] . In this environment, we install Python 3.8 and NumPy, and when we run the image it imports NumPy to make sure everything is working. This can bloat upto 950MB. Where is all the disk space being going? . Conda caches downloaded packages. | Conda base environment where toolchain is installed takes huge space. For example, when we install continuumio/miniconda3, it comes with its own python which we dont intend use. | First problem can be solved by removing those cached files. Second problem is conda specific and hence unavoidable but we can do away with it at runtime. . # The build-stage image: FROM continuumio/miniconda3 AS build # Install the package as normal: COPY environment.yml . RUN conda env create -f environment.yml # Install conda-pack: RUN conda install -c conda-forge conda-pack # Use conda-pack to create a standalone enviornment # in /venv: RUN conda-pack -n example -o /tmp/env.tar &amp;&amp; mkdir /venv &amp;&amp; cd /venv &amp;&amp; tar xf /tmp/env.tar &amp;&amp; rm /tmp/env.tar # We&#39;ve put venv in same path it&#39;ll be in final image, # so now fix up paths: RUN /venv/bin/conda-unpack # The runtime-stage image; we can use Debian as the # base image since the Conda env also includes Python # for us. FROM debian:buster AS runtime # Copy /venv from the previous stage: COPY --from=build /venv /venv # When image is run, run the code with the environment # activated: SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] ENTRYPOINT source /venv/bin/activate &amp;&amp; python -c &quot;import numpy; print(&#39;success!&#39;)&quot; . The above solutions is provided here. . . Follow me on twitter @theujjwal9 .",
            "url": "http://ujjwal9.ml/blog/docker/dl-tools/2020/12/24/docker.html",
            "relUrl": "/docker/dl-tools/2020/12/24/docker.html",
            "date": " ‚Ä¢ Dec 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Heartrate detection using camera",
            "content": "Euler Video Magnification Project Link at MIT CSAIL . Introuction . The world generates countless signals as it moves ahead in time, but most of these signals are invisible to the human eyes. This is due to the frequency of these signals, either it is too low to be perceived as change or it is too high that we can‚Äôt see the change actually occurring. . Finding a pattern in a seemingly random event is about how do we process the given information and convert it into something useful. In this article, we try to connect the dots of predicting heart rate with a camera alone. For this, we tag the video frame with actual heart rate and get to know which features impact an accurate prediction. For example which color signal in our RGB frames contributes towards better prediction and many more such small details. . Take for instance our eyes and our ability to understand facial expressions in others. These changes last in a long enough amount of time that we are able to perceive it. But changes exhibited by a heart rate last for a very short period of time and hence become invisible to us. With the camera of a sufficiently high frame rate, i.e almost all modern cameras, these changes become visible. And hence we can find the heart rate. . In this blog, we are going to introduce the algorithms required and demonstrate how to read a persons heart rate with a camera alone. The technique is called Euler Video Magnification. . Measuring Heart Rate . In order to measure your heart rate, doctors have traditionally relied upon technology that is based on monitors with leads that attach to your body. These devices measure one of the following pulses: . Radial Pulse: Place your pointer and middle fingers on the inside of your opposite wrist just below the thumb and then count how many beats you feel in 1 minute. . | Carotid Pulse: Place your pointer and middle fingers on the side of your windpipe just below the jawbone and then count how many beats you feel in 1 minute. . | Brachial Pulse: Another location for checking your pulse is the brachial artery. This method is used most commonly in young children. . | Overview of Euler video magnification . A computational technique for visualizing subtle color and motion variations in ordinary videos by making the variations larger. It is a microscope for small changes that are hard or impossible for us to see by ourselves. In addition, these small changes can be quantitatively analyzed and used to recover sounds from vibrations in distant objects, characterize material properties, and, in this case, remotely measure a person‚Äôs pulse. . Concept . The general concept behind this algorithm is to first of all approximate a point on the forehead. . The intensity of this point is then decomposed into different color space namely Red, Blue, Green. But we prefer Red and Green color only as Blue tends to introduce noise in heart rate detection. . . . The variation in Red and Green colorspace on the location approximated on the forehead is then fed to Fourier Transform to convert the function of spatial location on the video frame and time to frequency domain which therefore helps in extracting heart rate. . What is the Fourier transform? . The** Fourier transform** decomposes (also called analysis) a function of time (a signal) into its constituent frequencies. The Fourier transform of a function of time is itself a complex-valued function of frequency, whose magnitude component represents the amount of that frequency present in the original function, and whose complex argument is the phase offset of the basic sinusoid in that frequency. You can learn the basics of Fourier transform from this video. . . Now let‚Äôs talk about some amplification techniques namely Lagrangian and Eulerian. These techniques will help us in amplifying the particular frequency so that we can see change happening at that rate, in our case heart rate. . Lagrangian perspective . The Lagrangian version of amplification is to analyze the angle of motion of the pixels of interest in the tracking image. For example, if we want to study the flow rate of the river, we take a boat, go down the river, and record the movement of the ship. . However, the Lagrange perspective approach has the following shortcomings: . ‚ûî It is necessary to accurately track and estimate the trajectory of particles, which requires more computational resources. . ‚ûî The tracking of the particles is performed independently, and with the consideration, that system is closed i.e there is no transfer of energy in and out of frame being studied. So the lack of consideration of the overall image is prone to the fact that the image is not closed, thereby affecting the effect of the amplification. . ‚ûî The amplification of the action of the target object is to modify the motion trajectory of the particle. Since the position of the particle changes, it is necessary to fill the original position of the particle, as there is a continuous flow in action and system is closed therefore some other particle will take the position. This increases the complexity of the algorithm. . What is ‚Äúchange‚Äù ‚Äî the trajectory of the pixel of interest over time, such pixels often need to be assisted by manual or other prior knowledge; Amplify ‚Äúchange‚Äù ‚Äî increase the amplitude of these pixels. . Euler perspective . Unlike the Lagrangian perspective, the Euler version of amplification does not explicitly track and estimate the motion of the particle but instead fixes the perspective in one place, such as the entire image. . After that, it is assumed that the entire image is changing, but the characteristics of the signals like frequency, amplitude, etc. are varying. So we are interested in the change in the signals. In this way, the amplification of the ‚Äúchange‚Äù becomes the precipitation and enhancement of the frequency band of interest. For example, the same is to study the flow rate of river water. We can also sit on the shore and observe the change of the river when it passes through a fixed place. This change may contain many components that are not related to the water flow itself, such as the leaves falling off the water surface. Oh, but we only focus on the part that best reflects the water flow rate. . What is ‚Äúchange‚Äù ‚Äî the whole scene is changing, and the change signals we are interested in are hidden in it; Amplify ‚Äúchange‚Äù ‚Äî Separate and enhance the signal of interest by means of signal processing. . Explanation . Now, why are we able to extract heart rate from the sequence of the frame? It is because the heart pushes the blood to every part of the body and to the head particularly (towards the brain), so it changes the color and opacity of the skin. These changes can be detected by analyzing the average red or green component of the frames, taken from the camera. We learned the above concepts to be able to understand the different filters required to develop the said application. The analysis is done using the following approach : . Spatial filtering. Pyramid multiresolution decomposition of the video sequence; This is done to extract features/structures of interest, and to attenuate noise. . | Time domain filtering. Performing time-domain bandpass filtering on the images of each scale to obtain several frequency bands of interest; This is done using Fourier transform. . | Amplify the filtering result. The signal of each frequency band is differentially approximated by Taylor series, and the result of linear amplification is approximated; This is why we studied Euler amplification above. . | Composite image. The amplified image is synthesized. . | The spatial and temporal processing is used to emphasize subtle temporal changes in a video. . . The video sequence is decomposed into different spatial frequency bands. These bands might be magnified differently due to the difference in their SNR (Signal To Noise Ratio). The goal of spatial processing is simply to increase the temporal signal-to-noise ratio by pooling multiple pixels, then for the purpose of computational efficiency and spatial filtering, the low-pass filter is applied to the frames of the video spatially and then downsampled using Laplace Pyramid. . What is the Laplace Pyramid? . To understand this first of all we need to understand Gaussian Pyramid. . The original image is convolved with a Gaussian kernel. As described above the resulting image is a low pass filtered version of the original image. The cut-off frequency can be controlled using the parameter œÉ that is standard variation. . . The Laplacian is then computed as the difference between the original image and the low pass filtered image i.e it‚Äôs the difference between successive gaussian pyramid levels. This process is continued to obtain a set of band-pass filtered images (since each is the difference between two levels of the Gaussian pyramid). Thus the Laplacian pyramid is a set of bandpass filters . . The original image is repeatedly filtered and subsampled to generate the sequence of reduced resolution images. These comprise a set of low pass filtered copies of the original image in which the bandwidth decreases in one-octave steps. . So after using Laplace pyramid we then perform temporal processing on each spatial band. The time series corresponding to the value of a pixel in a frequency band is passed through a bandpass filter to extract the frequency bands of interest. For heart rate detection, I selected frequencies within 0.4‚Äì4Hz, corresponding to 24‚Äì240 beats per minute (this was specified in EVM paper itself) to magnify a pulse. The temporal processing is uniform for all spatial levels, and for all pixels within each level that is the time series of every pixel is passed through the same filter. . How is it done? . Spatial Filtering: It is done to spatially filter the video sequence to obtain basebands of different spatial frequencies. The purpose of spatial filtering is simply to ‚Äúspelt‚Äù multiple adjacent pixels into one piece, a low pass filter can be used. In fact, linear EVM uses Laplacian pyramids or Gaussian pyramids for multiresolution decomposition. . Time Domain Filtering: After obtaining the basebands of different spatial frequencies, bandpass filtering in the time domain is performed for each baseband in order to extract the part of the change signal we are interested in. For example, if we want to amplify the heart rate signal, we can choose bandpass filtering from 0.4 to 4 Hz (24 to 240 bpm). This band is the range of human heart rate. . Amplification : . I^(x,t)=f(x+(1+Œ±)Œ¥(t) hat I(x,t) = f(x+(1+ alpha) delta(t)I^(x,t)=f(x+(1+Œ±)Œ¥(t) . This represents Color Intensity of the pixels at location x in time t. Œ¥(t) represents the displacement function. Œ± is the amplification factor. . $f(x + Œ¥(t))$ in a first-order Taylor expansion about x, can be represented as: . I(x,t)‚âàf(x)+Œ¥(t)‚àÇf(x)‚àÇxI(x,t) approx f(x) + delta(t) frac{ partial f(x)}{ partial x}I(x,t)‚âàf(x)+Œ¥(t)‚àÇx‚àÇf(x)‚Äã . f(x)=f(a)+f‚Ä≤(a)(x‚àía)f(x) = f(a) + f&amp;#x27;(a)(x-a)f(x)=f(a)+f‚Ä≤(a)(x‚àía) . This is first order Taylor expansion. Here (x-a) is displacement function. . The temporal bandpass filter is selected to pull out the motions or signals that we wish to be amplified. . B(x,t)=Œ¥(t)‚àÇf(x)‚àÇxB(x,t) = delta(t) frac{ partial f(x)}{ partial x}B(x,t)=Œ¥(t)‚àÇx‚àÇf(x)‚Äã . This is a temporal Bandpass filter which is a result of applying a broadband temporal bandpass filter to $I(x, t)$ at every position x. For now, $ delta(t)$, is within the passband of the temporal bandpass filter. . Tip: For color amplification of blood flow, a narrow passband produces a more noise-free result. . . Important: The Butterworth filter is used to convert a user-specified frequency band into a second-order IIR (infinite impulse response) and is used in our real-time application. . This shows that the frequency response of some of the temporal filters used in the paper. Ideal bandpass filters are used for color amplification as they have passbands with sharp cutoff frequencies. . For pulse detection, after computing Laplacian pyramid the magnification value or amplification factor Œ±, for the finest two levels are set to 0. This causes downsampling and applies a spatial low pass filter to each frame to reduce both quantization and noise and to boost the subtle pulse signal that we are interested in. The incoming video frame is then passed through an ideal bandpass filter with a passband of 0.83 Hz to 1 Hz (50 bpm to 60 bpm). Finally, a large value of Œ± ‚âà 100 (amplification factor) and Œªc ‚âà 1000 (cutoff frequency, beyond which an attenuated version of Œ± is used that is either force Œ± to zero for all Œª &lt; Œªc, or linearly scale Œ± down to zero. This is important parameter in controlling noise) was applied to the resulting spatially lowpass signal to emphasize the color change as much as possible. The final video was formed by adding this signal back to the original. . . In this video, we can see periodic green to red variations at the heartbeat and how blood perfuses the face. . ‚ÄúHigher Œ± can exaggerate specific motions or color changes at the cost of increased noise.‚Äù In some cases, one can account for color clipping artifacts by attenuating the chrominance components of each frame. This approach achieves this by doing all the processing in the YIQ space. Users can attenuate the chrominance components, I and Q, before conversion to the original color space. . ‚ÄúThe paper Eulerian Video Magnification for Revealing Subtle Changes in the World is the work of the MIT CSAIL team.‚Äù . Conclusion . This algorithm can find its use in many aspects of our life like in pulse oximetry which is limited to certain application areas (usually the fingertip) and bears the risk of the probe failing due to the movement of the patient or low perfusion of the hands during long-time recordings. These limitations can be overcome by the analysis of video signals using this technique, which do not depend on contact-based measurement hardware and can be applied to well-circulated body areas (e.g. the head). . The world‚Äôs expanding and aging population has created a demand for inexpensive, unobtrusive, automated healthcare solutions. Eulerian Video Magnification (EVM) aids in the development of these solutions by allowing for the extraction of physiological signals from video data. This paper examines the potential of thermal video in conjunction with EVM to extract physiological measures, particularly heart rate. . . Follow me on twitter @theujjwal9 .",
            "url": "http://ujjwal9.ml/blog/maths/video/2019/04/06/heartrate-detection.html",
            "relUrl": "/maths/video/2019/04/06/heartrate-detection.html",
            "date": " ‚Ä¢ Apr 6, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Knowledge Distillation",
            "content": "The blog first appeared at Intel Devpost. Here is the link . Hinton, Geoffrey, et al. ‚ÄúDistilling the Knowledge in a Neural Network.‚Äù arXiv, 9 Mar. 2015, arxiv.org/abs/1503.02531v1. . Link to paper . Abstract . The problem that we are facing right now is that we have built sophisticated models that can perform complex tasks, but the question is, how do we deploy such bulky models on our mobile devices for instant usage. Obviously, we can deploy our model to the cloud and can call it whenever we need its service but this would require a reliable internet connection and hence it becomes a constraint in production. So what we need is a model that can run on our mobile devices. . . **So what‚Äôs the problem? **We can train a small network that can run on the limited computational resource of our mobile device. But there is a problem in this approach. Small models can‚Äôt extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Though ensemble of small models gives good results but unfortunately making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users. In this case, we resort to either of the 2 techniques: . Knowledge Distillation . | Model Compression . | If you have developed a better solution or if I might have missed something, please mention in the comments üôÇ . | . In this blog, we will look at Knowledge Distillation. I will cover model compression in an upcoming blog. . So knowledge distillation is a simple way to improve the performance of deep learning models on mobile devices. In this process, we train a large and complex network or an ensemble model which can extract important features from the given data and can, therefore, produce better predictions. Then we train a small network with the help of the cumbersome model. This small network will be able to produce comparable results, and in some cases, it can even be made capable of replicating the results of the cumbersome network. . . For example, Since GoogLeNet is a very cumbersome (means deep and complex) network, its deepness gives the ability to extract and complex features and its complexity gives it the power to remain accurate. But the model is heavy enough that one for sure need a large amount of memory and a powerful GPU to perform large and complex calculations. So that‚Äôs why we need to transfer the knowledge learned by this model to a much smaller model which can easily be used in mobile. . About Cumbersome Models . Cumbersome models learn to discriminate between a large number of classes. The normal training objective is to maximize the average log probability of the correct answer, and it assigns a probability to all the classes, with some classes given small probabilities with respect to others. The relative probabilities of incorrect answers tell us a lot about how this complex model tends to generalize. An image of a Car, for example, may only have a very small chance of being mistaken for a Truck, but that mistake is still many times more probable than mistaking it for a Cat. . Note that objective function should be chosen such that it generalizes well to new data. So it should be kept in mind while selecting an appropriate objective function that it shouldn‚Äôt be selected in such a way that it optimizes well on training data. . Since these operations will be quite heavy for mobile during the performance, so to deal with this situation, we have to transfer the knowledge of the cumbersome model to a small model which can be easily exported to mobile devices. To achieve this, we can consider the cumbersome model as Teacher Network and our new small model as Student Network. . Teacher and Student . You can ‚Äòdistill‚Äô the large and complex network in another much smaller network, and the smaller network does a reasonable job of approximating the original function learned by a deep network. . However, there is a catch, the distilled model (student), is trained to mimic the output of the larger network (teacher), instead of training it on the raw data directly. This has something to do with how the deeper network learns hierarchical abstractions of the features. . So how is this transfer of knowledge done? . ![](https://cdn-images-1.medium.com/max/2964/1*WxFiH3XDY1-28tbyi4BGDA.png) The transferring of the generalization ability of the cumbersome model to a small model can be done by the use of class probabilities produced by the cumbersome model as ‚Äúsoft targets‚Äù for training the small model. For this transfer stage, we use the same training set or a separate ‚Äútransfer‚Äù set as used for training the cumbersome model. When the cumbersome model is a large ensemble of simpler models, we can use arithmetic or geometric mean of their individual predictive distributions as the soft targets. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate. . Much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2‚Äôs look like 3‚Äôs and which look like 7‚Äôs or which ‚Äúgolden retriever‚Äù looks like ‚ÄúLabrador‚Äù) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero. . Distillation . For distilling the learned knowledge we use Logits (the inputs to the final softmax). Logits can be used for learning the small model and this can be done by minimizing the squared difference between the logits produced by the cumbersome model and the logits produced by the small model. . . For high temperatures (T -&gt; inf), all actions have nearly the same probability and at the lower the temperature (T -&gt; 0), the more expected rewards affect the probability. For low temperature, the probability of the action with the highest expected reward tends to 1. . In distillation, we raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. We then use the same high temperature when training the small model to match these soft targets. . Objective Function . The first objective function is the cross-entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model. . The second objective function is the cross-entropy with the correct labels and this is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1 . Training ensembles of specialists . Training an ensemble of models is a very simple way to take advantage of parallel computation. But there is an objection that an ensemble requires too much computation at test time. But this can be easily dealt with the technique we are learning. And so ‚ÄúDistillation‚Äù can be used to deal with this allegation. . Specialist Models . Specialist models and one generalist model make our one cumbersome model. Generalist Model is trained on all training data and Specialist Models focus on a different confusable subset of the classes can reduce the total amount of computation required to learn an ensemble. The main problem with specialists is that they overfit very easily. But this overfitting may be prevented by using soft targets. . Reduce Overfitting in Specialist Models . To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist, with half its examples coming from its special subset, and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled. . Assign classes to Specialists . We apply a clustering algorithm to the covariance matrix of the predictions of our generalist model so that a set of classes Sm that are often predicted together will be used as targets for one of our specialist models, m. So we apply K-means clustering to the columns of the covariance matrix to get our required clusters or classes. . . Covariance/Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance. . Performing inference . For each test case, we find the ‚Äòn‚Äô most probable classes according to the generalist model. Call this set of classes k. . | We then take all the specialist models, m, whose special subset of confusable classes, Sm, has a non-empty intersection with k and call this the active set of specialists Ak (note that this set may be empty). We then find the full probability distribution q over all the classes that minimizes: . | . KL(pg,q)+‚àëmœµAkKL(pm,q)KL(p^g, q) + sum_{m epsilon A_k} KL(p^m, q)KL(pg,q)+mœµAk‚Äã‚àë‚ÄãKL(pm,q) . KL(p‚à£‚à£q)=‚àëipilog‚Å°piqiKL(p||q) = sum_{i}p_i log frac{p_i}{q_i}KL(p‚à£‚à£q)=i‚àë‚Äãpi‚Äãlogqi‚Äãpi‚Äã‚Äã . KL denotes the KL divergence, and $p^m$, $p^g$ denote the probability distribution of a specialist model or the generalist full model. The distribution $p^m$ is over all the specialist classes of $m$ plus a single dustbin class, so when computing its $KL$ divergence from the full $q$ distribution we sum all of the probabilities that the full $q$ distribution assigns to all the classes in $m$‚Äôs dustbin. . Soft Targets as Regularizers . Soft Targets or labels predicted from a model contain more information that binary hard labels due to the fact that they encode similarity measures between the classes. . Incorrect labels tagged by the model describe co-label similarities, and these similarities should be evident in future stages of learning, even if the effect is diminished. For example, imagine training a deep neural net on a classification dataset of various dog breeds. In the initial few stages of learning the model will not accurately distinguish between similar dog-breeds such as a Belgian Shepherd versus a German Shepherd. This same effect, although not so exaggerated, should appear in later stages of training. If given an image of a German Shepherd, the model predicts the class German Shepherd with a high-accuracy, the next highest predicted dog should still be a Belgian Shepherd or a similar looking dog. Over-fitting starts to occur when the majority of these co-label effects begin to disappear. By forcing the model to contain these effects in the later stages of training, we reduced the amount of over-fitting. . Though using soft targets as Regularizers is not considered very effective. . Associated Code can be found at Github. . . Follow me on twitter @theujjwal9 .",
            "url": "http://ujjwal9.ml/blog/dl/distillation/2018/05/04/knowledge-distillation.html",
            "relUrl": "/dl/distillation/2018/05/04/knowledge-distillation.html",
            "date": " ‚Ä¢ May 4, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am researcher exploring domains of Artificial Intelligence. My interest is at the intersection of Computer Vision and Representation Learning. My primary focus is to build systems that offer true disentangled abstraction of human psychology and challenges human consciousness. I plan to pursue my vision for such systems by extending my approach to reinforcement learning and non-differentiable intelligence. . Webpage: ujjwal9.ml . Twitter: @theujjwal9 .",
          "url": "http://ujjwal9.ml/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "http://ujjwal9.ml/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}